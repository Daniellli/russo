{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import os.path as osp\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "os.chdir(osp.dirname(os.getcwd()))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from glob import glob\n",
    "# from my_script.utils import readtxt\n",
    "# consistency_list_qualitative_res_path = \"/data3/xusc/exp/butd_detr/logs\"\n",
    "\n",
    "# all_data = {}\n",
    "\n",
    "# for f in glob(consistency_list_qualitative_res_path+\"/scanrefer20*\" ):\n",
    "#     data = np.loadtxt(f,delimiter='\\n',dtype=np.str0).tolist()\n",
    "#     name = f.split('/')[-1].split('.')[0]\n",
    "#     print(len(data),name)\n",
    "#     all_data[name] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick_samples = []\n",
    "# for x in all_data['scanrefer20_all_consistenct_loss']:\n",
    "#     if x not in  all_data['scanrefer20_no_constrastive_soft_token_consistency_loss_qualitative_list'] and \\\n",
    "#         x not in  all_data['scanrefer20_no_contrastive_consistency_loss_qualitative_list'] and \\\n",
    "#             x not in  all_data['scanrefer20_no_soft_token_consistency_loss_qualitative_list']:\n",
    "#             pick_samples.append(x)\n",
    "\n",
    "# len(pick_samples)\n",
    "# np.savetxt(osp.join(consistency_list_qualitative_res_path,'only_all_consistency_works_qua_list.txt'),pick_samples,fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "'''\n",
    "description:  从SR3D作者那获取 详细的标注数据\n",
    "param {*} split_name\n",
    "return {*}\n",
    "'''\n",
    "def get_refer_it_3D(data_name,split=None):\n",
    "\n",
    "    \n",
    "    scanrefer_root=\"datasets/refer_it_3d\"\n",
    "\n",
    "    if split is None :\n",
    "        data = pd.read_csv(osp.join(scanrefer_root,data_name+\".csv\"))\n",
    "    else :\n",
    "        data = pd.read_csv(osp.join(scanrefer_root,data_name+\"_\"+split+\".csv\"))\n",
    "\n",
    "    # logger.info(f\"len of {split_name} : {data.shape[0]}\")\n",
    "    # all_attrs = data.columns\n",
    "    # logger.info(f\" column of {split_name} : {all_attrs}\")\n",
    "    # logger.info(f\"scene number : {len(set(data['scan_id']))}\")\n",
    "    # stat = Counter(data['scan_id'])\n",
    "    # scane_stat = np.array([v for k ,v in stat.items()])\n",
    "    # avg_sample =scane_stat.mean()\n",
    "    # min_sample =scane_stat.min()\n",
    "    # max_sample =scane_stat.max()\n",
    "\n",
    "    # logger.info(f\"min sample: {min_sample} \\n max sample : {max_sample} \\n avg sample: {avg_sample}\")    \n",
    "    # print(data.iloc[0,:])\n",
    "    return data\n",
    "\n",
    "def read_txt(file_name):\n",
    "    with open(file_name,'r') as f:\n",
    "        scan_ids = f.read().split(\"\\n\")\n",
    "    logger.info(f\" len of {file_name} : {len(scan_ids)}\")\n",
    "    return scan_ids\n",
    "\n",
    "      \n",
    "\n",
    "def save_txt(path,data):\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 00:07:01.295 | INFO     | __main__:get_ratio_split_list:99 -  assignment id length : 3291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 00:07:23.296 | INFO     | __main__:get_all_ann:128 - annos num = 2884\n",
      "2022-11-15 00:07:23.307 | INFO     | __main__:get_ratio_split_list:99 -  assignment id length : 6583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 00:07:57.690 | INFO     | __main__:get_all_ann:128 - annos num = 5498\n",
      "2022-11-15 00:07:57.706 | INFO     | __main__:get_ratio_split_list:99 -  assignment id length : 9875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "0.30000000000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 00:08:43.748 | INFO     | __main__:get_all_ann:128 - annos num = 7749\n",
      "2022-11-15 00:08:43.767 | INFO     | __main__:get_ratio_split_list:99 -  assignment id length : 13167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 00:09:40.545 | INFO     | __main__:get_all_ann:128 - annos num = 9920\n",
      "2022-11-15 00:09:40.568 | INFO     | __main__:get_ratio_split_list:99 -  assignment id length : 16459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 00:10:47.250 | INFO     | __main__:get_all_ann:128 - annos num = 11756\n",
      "2022-11-15 00:10:47.276 | INFO     | __main__:get_ratio_split_list:99 -  assignment id length : 19751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 00:12:02.831 | INFO     | __main__:get_all_ann:128 - annos num = 13614\n",
      "2022-11-15 00:12:02.860 | INFO     | __main__:get_ratio_split_list:99 -  assignment id length : 23043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "0.7000000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 00:13:27.283 | INFO     | __main__:get_all_ann:128 - annos num = 15122\n",
      "2022-11-15 00:13:27.315 | INFO     | __main__:get_ratio_split_list:99 -  assignment id length : 26335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 00:14:59.546 | INFO     | __main__:get_all_ann:128 - annos num = 16470\n",
      "2022-11-15 00:14:59.581 | INFO     | __main__:get_ratio_split_list:99 -  assignment id length : 29627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 00:16:38.988 | INFO     | __main__:get_all_ann:128 - annos num = 17926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class ClsEntity:\n",
    "\n",
    "    def __init__(self,data_name, split,ratio=None):\n",
    "\n",
    "        \n",
    "        self.data_name=data_name\n",
    "        self.split=split\n",
    "        self.ratio=ratio\n",
    "\n",
    "        scan_ids=None\n",
    "        if ratio is None:\n",
    "            scan_ids=self.get_split_list()\n",
    "        else :\n",
    "            scan_ids=self.get_ratio_split_list()\n",
    "\n",
    "        self.scan_ids = scan_ids\n",
    "\n",
    "        self.get_all_ann()\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    '''\n",
    "    description:  获取SR3D 作者划分好的 训练集 和测试集\n",
    "    param {*} split\n",
    "    return {*}\n",
    "    '''\n",
    "    def get_split_list(self):\n",
    "        with open('data/meta_data/%s_%s_scans.txt' % (self.data_name,self.split)) as f:\n",
    "            scan_ids = set(eval(f.read()))\n",
    "        logger.info(f\" length : {len(scan_ids)}\")\n",
    "        return scan_ids\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def get_ratio_split_list(self):\n",
    "        with open('data/meta_data/%s_%s_%.1f.txt' % (self.data_name,self.split,self.ratio)) as f:\n",
    "            scan_ids = f.read().split(\"\\n\")\n",
    "        logger.info(f\" length : {len(scan_ids)}\")\n",
    "        return scan_ids\n",
    "\n",
    "\n",
    "\n",
    "    def get_all_ann(self):\n",
    "\n",
    "      raise NotImplemented\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "class SR3DEntity(ClsEntity):\n",
    "\n",
    "\n",
    "    def __init__(self,data_name, split,ratio=None):\n",
    "        super().__init__(data_name, split,ratio)\n",
    "\n",
    "    \n",
    "    def get_all_ann(self):\n",
    "        annos=  get_refer_it_3D(self.data_name,self.split)\n",
    "\n",
    "        #* 根据当前scenario 进行过滤\n",
    "        ans = []\n",
    "        for idx in range(annos.shape[0]):\n",
    "\n",
    "            ann = annos.iloc[idx]\n",
    "\n",
    "            if ann.scan_id in self.scan_ids and ann['mentions_target_class'].lower() == 'true':\n",
    "                ans.append(ann)\n",
    "            \n",
    "        self.annos = ans\n",
    "        logger.info(f\"annos num = {len(self.annos)}\")\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "class NR3DEntity(ClsEntity):\n",
    "\n",
    "    def __init__(self,data_name, split,ratio=None):\n",
    "        super().__init__(data_name, split,ratio)\n",
    "        \n",
    "\n",
    "    '''\n",
    "    description:  获取SR3D 作者划分好的 训练集 和测试集\n",
    "    param {*} split\n",
    "    return {*}\n",
    "    '''\n",
    "    def get_split_list(self):\n",
    "        with open('data/meta_data/%s_%s_scans.txt' % (self.data_name,self.split)) as f:\n",
    "            scan_ids = set(eval(f.read()))\n",
    "        logger.info(f\" length : {len(scan_ids)}\")\n",
    "        return scan_ids\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "    def get_ratio_split_list(self):\n",
    "        with open('data/meta_data/%s_%s_%.1f.txt' % (self.data_name,self.split,self.ratio)) as f:\n",
    "            scan_ids = f.read().split(\"\\n\")\n",
    "        logger.info(f\" assignment id length : {len(scan_ids)}\")\n",
    "        return np.array(scan_ids,dtype=np.int64).tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_all_ann(self):\n",
    "        annos=  get_refer_it_3D(self.data_name)\n",
    "\n",
    "        #* 根据当前scenario 进行过滤            \n",
    "        ans = []\n",
    "        for idx in range(annos.shape[0]):\n",
    "            ann = annos.iloc[idx]\n",
    "            #* 根据assignment id 来划分数据集\n",
    "            if self.ratio is not None:\n",
    "                if ann.assignmentid in self.scan_ids \\\n",
    "                    and str(ann['mentions_target_class']).lower() == 'true' \\\n",
    "                        and (str(ann['correct_guess']).lower() == 'true' or self.split != 'test'):\n",
    "                    ans.append(ann)\n",
    "\n",
    "            #* 根据 scan id 来划分数据集\n",
    "            else:\n",
    "                if ann.scan_id in self.scan_ids \\\n",
    "                    and str(ann['mentions_target_class']).lower() == 'true' \\\n",
    "                        and (str(ann['correct_guess']).lower() == 'true' or self.split != 'test'):\n",
    "                    ans.append(ann)\n",
    "                     \n",
    "            \n",
    "        self.annos = ans\n",
    "        logger.info(f\"annos num = {len(self.annos)}\")\n",
    "\n",
    "            \n",
    "\n",
    "    def generate_NR3D_labeled_scene_txt(self,labeled_ratio):\n",
    "\n",
    "        nr3d_ids = self.get_split_list(dataset_name='nr3d')\n",
    "\n",
    "        num_scans = len(nr3d_ids)\n",
    "        num_labeled_scans = int(num_scans*labeled_ratio)\n",
    "\n",
    "\n",
    "        choices = np.random.choice(num_scans, num_labeled_scans, replace=False)#* 从num_scans 挑选num_labeled_scans 个场景 出来 \n",
    "\n",
    "        labeled_scan_names = list(np.array(list(nr3d_ids))[choices])\n",
    "        \n",
    "        save_txt(os.path.join('data/meta_data/nr3d_train_{}.txt'.format(labeled_ratio)),'\\n'.join(labeled_scan_names))\n",
    "    \n",
    "        logger.info('\\tSelected {} labeled scans, remained {} unlabeled scans'.format(len(labeled_scan_names),num_scans- len(labeled_scan_names)))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# sr3d_ids = get_split_list()\n",
    "# sr3d_ids_test = get_split_list(split = 'test')\n",
    "# nr3d_ids = get_split_list(dataset_name='nr3d')\n",
    "# nr3d_ids_test = get_split_list(dataset_name='nr3d',split = 'test')\n",
    "# get_refer_it_3D('sr3d+')\n",
    "# get_refer_it_3D('sr3d')\n",
    "# get_refer_it_3D('sr3d_test')\n",
    "# get_refer_it_3D('sr3d_train')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "# a = ClsEntity('sr3d','train',0.2)\n",
    "# a = ClsEntity('sr3d','test')\n",
    "# a = NR3DEntity('nr3d','train',0.1)\n",
    "# a = NR3DEntity('nr3d','train')\n",
    "# a = NR3DEntity('nr3d','test')\n",
    "\n",
    "\n",
    "\n",
    "for ratio in np.linspace(0.1,0.9,9):\n",
    "    logger.info(ratio)\n",
    "    NR3DEntity('nr3d','train',round(ratio,1))\n",
    "    logger.info(\"==================================================================\")\n",
    "\n",
    "\n",
    "\n",
    "# for ratio in np.linspace(0.1,0.9,9):\n",
    "#     logger.info(ratio)\n",
    "#     SR3DEntity('nr3d','train',round(ratio,1))\n",
    "#     logger.info(\"==================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_scene_data(all_data,scene = 'scene0525_00'):\n",
    "    \n",
    "    return_data = [] \n",
    "    for  idx in range(all_data.shape[0]):\n",
    "\n",
    "        if all_data.iloc[idx]['scan_id'] == scene:\n",
    "            return_data.append(all_data.iloc[idx])\n",
    "            \n",
    "\n",
    "    return return_data\n",
    "\n",
    "\n",
    "def get_assignment_id(all_data,scene_list):\n",
    "    assignments = []\n",
    "    for idx in range(all_data.shape[0]):\n",
    "        if all_data.iloc[idx]['scan_id'] in scene_list: \n",
    "            assignments.append (all_data.iloc[idx]['assignmentid'])\n",
    "        \n",
    "    return assignments\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_labeled_according_assignment_id(assign_ids , ratio=0.4):\n",
    "    \n",
    "    length = len(assign_ids)\n",
    "    choices = np.random.choice(length,int(length*ratio))\n",
    "\n",
    "    return np.array(assign_ids)[choices]\n",
    "\n",
    "\n",
    "def split_nr3d_according_to_assignmentid():\n",
    "    for ratio in np.linspace(0.1,0.9,9):\n",
    "        ratio = round(ratio,1)\n",
    "        split_labeled_data = split_labeled_according_assignment_id(nr3d_all_assign_ids,ratio)\n",
    "        print(f\"length  = {len(split_labeled_data)}\")\n",
    "        save_txt(os.path.join('data/meta_data/nr3d_train_{}.txt'.format(ratio)),'\\n'.join(split_labeled_data.astype(np.str0).tolist()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stat_mention_data_nr3d(nr3d_all_data,ratio=0.2):\n",
    "    data = read_txt('data/meta_data/nr3d_train_{}.txt'.format(ratio))\n",
    "    sum_=0\n",
    "    for idx in range(nr3d_all_data.shape[0]):\n",
    "        # if str(nr3d.iloc[idx]['assignmentid']) in data and nr3d.iloc[idx]['mentions_target_class']  and nr3d.iloc[idx]['correct_guess'] :\n",
    "        if str(nr3d_all_data.iloc[idx]['assignmentid']) in data and nr3d_all_data.iloc[idx]['mentions_target_class']:\n",
    "            sum_+=1\n",
    "    logger.info(sum_)\n",
    "\n",
    "# nr3d = get_refer_it_3D('nr3d')\n",
    "# train_split = get_split_list(dataset_name='nr3d')\n",
    "# nr3d_all_assign_ids = get_assignment_id(nr3d,train_split)\n",
    "# save_txt(os.path.join('data/meta_data/nr3d_train_all_assignmentid.txt'),'\\n'.join(np.array(nr3d_all_assign_ids).astype(np.str0).tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_scanrefer(split=None):\n",
    "    if split is not None :\n",
    "        path = \"datasets/scanrefer/ScanRefer_filtered_%s.json\"%split\n",
    "    else :\n",
    "        path = \"datasets/scanrefer/ScanRefer_filtered.json\"\n",
    "\n",
    "    with open (path,'r')as f :\n",
    "        data =json.load(f)\n",
    "    \n",
    "    length = len(data)\n",
    "    logger.info(f\" len of {split} split : {length}\")\n",
    "    # all_scene = set([x['scene_id']  for x in data])\n",
    "    # logger.info(f\" scene number  of {split} split : {len(all_scene)}\")\n",
    "\n",
    "    # all_object_id = set([x['object_id']  for x in data])\n",
    "    # logger.info(f\" object number  of {split} split : {len(all_object_id)}\")\n",
    "\n",
    "    # all_anno_id = set([x['ann_id']  for x in data])\n",
    "    # logger.info(f\" anno number  of {split} split : {len(all_anno_id)}\")\n",
    "\n",
    "    # print(data[0])\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_ratio_scanrefer(ratio,split=None):\n",
    "    \n",
    "    path = \"datasets/scanrefer/ScanRefer_filtered_train_%.1f.txt\"%(ratio)\n",
    "\n",
    "    scanrefer = get_scanrefer(split='train')\n",
    "    scan_ids = read_txt(path)\n",
    "\n",
    "    \n",
    "    num = 0 \n",
    "    for refer in scanrefer:\n",
    "        if refer['scene_id'] in scan_ids:\n",
    "            num+=1\n",
    "    \n",
    "    print(f\"len of scanrefer {ratio} : {num}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "description: \n",
    "param {*} labeled_ratio\n",
    "return {*}\n",
    "'''\n",
    "def generate_scanrefer_labeled_scene_txt(labeled_ratio):\n",
    "    all_scenes = get_scanrefer(split='train')\n",
    "\n",
    "    num_scans = len(all_scenes)\n",
    "    num_labeled_scans = int(num_scans*labeled_ratio)\n",
    "\n",
    "\n",
    "    choices = np.random.choice(num_scans, num_labeled_scans, replace=False)#* 从num_scans 挑选num_labeled_scans 个场景 出来 \n",
    "\n",
    "    labeled_scan_names = list(np.array(list(all_scenes))[choices])\n",
    "    \n",
    "    with open(os.path.join('datasets/scanrefer/ScanRefer_filtered_train_{}.txt'.format(labeled_ratio)), 'w') as f:\n",
    "        f.write('\\n'.join(labeled_scan_names))\n",
    "    \n",
    "    logger.info('\\tSelected {} labeled scans, remained {} unlabeled scans'.format(len(labeled_scan_names),num_scans- len(labeled_scan_names)))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "'''\n",
    "description:  一次性生成所有比例的数据集划分\n",
    "return {*}\n",
    "'''\n",
    "def generate_ratio_labeled_datasets():\n",
    "    for x in np.linspace(0.1,0.9,9):\n",
    "        generate_NR3D_labeled_scene_txt(round(x,1))\n",
    "    for x in np.linspace(0.1,0.9,9):\n",
    "        generate_scanrefer_labeled_scene_txt(round(x,1))\n",
    "\n",
    "'''\n",
    "description:  tmp code for stat \n",
    "return {*}\n",
    "'''\n",
    "def stat():\n",
    "    datasets = ['sr3d','nr3d']\n",
    "\n",
    "    for dataset in datasets:\n",
    "        for n in np.linspace(0.1,0.9,9):\n",
    "            # print(round(n,1))\n",
    "            statstic_nr3d_by_ratio(round(n,1),dataset)\n",
    "            \n",
    "    for n in np.linspace(0.1,0.9,9):\n",
    "        get_ratio_scanrefer(round(n,1))\n",
    "        \n",
    "\n",
    "\n",
    "# scan_val =get_scanrefer(split='val')\n",
    "\n",
    "\n",
    "# get_scanrefer(split='test')\n",
    "# get_scanrefer()\n",
    "# scanrefer = get_scanrefer(split='train')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scanrefer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1785247/3171340516.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mscene_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"scene0000_00\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mscene_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_scanrefer_data_by_scene\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscanrefer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscene_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"len of scene : {len(scene_data)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scanrefer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_scanrefer_data_by_scene(all_data,scene_name):\n",
    "    \n",
    "    res = [] \n",
    "    for line in all_data:\n",
    "        # print(line['scene_id'])\n",
    "        # print(line['object_id'])\n",
    "        # print(line['object_name'])\n",
    "        # print(line['ann_id'])\n",
    "\n",
    "        # for k,v in line.items():\n",
    "        #     print(k)\n",
    "        if line['scene_id'] == scene_name:\n",
    "            res.append(line)\n",
    "    return res\n",
    "\n",
    "scene_name = \"scene0000_00\"\n",
    "scene_data = get_scanrefer_data_by_scene(scanrefer,scene_name)\n",
    "print(f\"len of scene : {len(scene_data)}\")\n",
    "print(scene_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of scene : 173\n",
      "{'scene_id': 'scene0000_00', 'object_id': '39', 'object_name': 'cabinet', 'ann_id': '1', 'description': 'a white cabinet in the corner of the room. in the direction from the door and from the inside . it will be on the left, there is a small brown table on the left side of the cabinet and a smaller table on the right side of the cabinet', 'token': ['a', 'white', 'cabinet', 'in', 'the', 'corner', 'of', 'the', 'room', '.', 'in', 'the', 'direction', 'from', 'the', 'door', 'and', 'from', 'the', 'inside', '.', 'it', 'will', 'be', 'on', 'the', 'left', ',', 'there', 'is', 'a', 'small', 'brown', 'table', 'on', 'the', 'left', 'side', 'of', 'the', 'cabinet', 'and', 'a', 'smaller', 'table', 'on', 'the', 'right', 'side', 'of', 'the', 'cabinet']}\n",
      "object_39 : 5\n",
      "ann_1 : 34\n",
      "object_39_ann_1 : 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def stat_object_id_for_scene(scene_data,object_id):\n",
    "    res = [] \n",
    "    for line in scene_data:\n",
    "        if line['object_id'] == object_id:\n",
    "            res.append(line)\n",
    "    return res\n",
    "\n",
    "def stat_ann_id_for_scene(scene_data,ann_id):\n",
    "    res = [] \n",
    "    for line in scene_data:\n",
    "        if line['ann_id'] == ann_id:\n",
    "            res.append(line)\n",
    "    return res\n",
    "\n",
    "def stat_ann_id_and_object_id_for_scene(scene_data,ann_id,object_id):\n",
    "    res = [] \n",
    "    for line in scene_data:\n",
    "        if line['ann_id'] == ann_id and line['object_id'] == object_id:\n",
    "            res.append(line)\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "# object_39 = stat_object_id_for_scene(scene_data,'39')\n",
    "# ann_1=stat_ann_id_for_scene(scene_data,'1')\n",
    "# object_39_ann_1 = stat_ann_id_and_object_id_for_scene(scene_data,'1','39')\n",
    "# print(f\"object_39 : {len(object_39)}\")\n",
    "# print(f\"ann_1 : {len(ann_1)}\")\n",
    "# print(f\"object_39_ann_1 : {len(object_39_ann_1)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('cerberus2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6c611ea97879ac7a66cc839a92507d3b630293601f06de205ed521c4e63823d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
