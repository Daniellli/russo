{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import os.path as osp\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "os.chdir(osp.dirname(os.getcwd()))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from glob import glob\n",
    "# from my_script.utils import readtxt\n",
    "# consistency_list_qualitative_res_path = \"/data3/xusc/exp/butd_detr/logs\"\n",
    "\n",
    "# all_data = {}\n",
    "\n",
    "# for f in glob(consistency_list_qualitative_res_path+\"/scanrefer20*\" ):\n",
    "#     data = np.loadtxt(f,delimiter='\\n',dtype=np.str0).tolist()\n",
    "#     name = f.split('/')[-1].split('.')[0]\n",
    "#     print(len(data),name)\n",
    "#     all_data[name] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick_samples = []\n",
    "# for x in all_data['scanrefer20_all_consistenct_loss']:\n",
    "#     if x not in  all_data['scanrefer20_no_constrastive_soft_token_consistency_loss_qualitative_list'] and \\\n",
    "#         x not in  all_data['scanrefer20_no_contrastive_consistency_loss_qualitative_list'] and \\\n",
    "#             x not in  all_data['scanrefer20_no_soft_token_consistency_loss_qualitative_list']:\n",
    "#             pick_samples.append(x)\n",
    "\n",
    "# len(pick_samples)\n",
    "# np.savetxt(osp.join(consistency_list_qualitative_res_path,'only_all_consistency_works_qua_list.txt'),pick_samples,fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-14 22:46:51.776 | INFO     | __main__:get_ratio_split_list:16 -  len of ('sr3d', 'train', 0.1) : 101\n",
      "2022-11-14 22:46:51.778 | INFO     | __main__:get_ratio_split_list:16 -  len of ('sr3d', 'train', 0.2) : 203\n",
      "2022-11-14 22:46:51.779 | INFO     | __main__:get_ratio_split_list:16 -  len of ('sr3d', 'train', 0.3) : 305\n",
      "2022-11-14 22:46:51.780 | INFO     | __main__:get_ratio_split_list:16 -  len of ('sr3d', 'train', 0.4) : 407\n",
      "2022-11-14 22:46:51.781 | INFO     | __main__:get_ratio_split_list:16 -  len of ('sr3d', 'train', 0.5) : 509\n",
      "2022-11-14 22:46:51.782 | INFO     | __main__:get_ratio_split_list:16 -  len of ('sr3d', 'train', 0.6) : 610\n",
      "2022-11-14 22:46:51.783 | INFO     | __main__:get_ratio_split_list:16 -  len of ('sr3d', 'train', 0.7) : 712\n",
      "2022-11-14 22:46:51.784 | INFO     | __main__:get_ratio_split_list:16 -  len of ('sr3d', 'train', 0.8) : 814\n",
      "2022-11-14 22:46:51.785 | INFO     | __main__:get_ratio_split_list:16 -  len of ('sr3d', 'train', 0.9) : 916\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "description:  获取SR3D 作者划分好的 训练集 和测试集\n",
    "param {*} split\n",
    "return {*}\n",
    "'''\n",
    "def get_split_list(dataset_name=\"sr3d\" ,split='train'):\n",
    "    with open('data/meta_data/%s_%s_scans.txt' % (dataset_name,split)) as f:\n",
    "        scan_ids = set(eval(f.read()))\n",
    "    logger.info(f\" len of {dataset_name,split} : {len(scan_ids)}\")\n",
    "    return scan_ids\n",
    "\n",
    "\n",
    "def get_ratio_split_list(ratio,dataset_name=\"sr3d\" ,split='train'):\n",
    "\n",
    "    with open('data/meta_data/%s_%s_%.1f.txt' % (dataset_name,split,ratio)) as f:\n",
    "        scan_ids = f.read().split(\"\\n\")\n",
    "\n",
    "    logger.info(f\" len of {dataset_name,split,ratio} : {len(scan_ids)}\")\n",
    "    \n",
    "    return scan_ids\n",
    "\n",
    "\n",
    "def read_txt(file_name):\n",
    "    with open(file_name,'r') as f:\n",
    "        scan_ids = f.read().split(\"\\n\")\n",
    "    logger.info(f\" len of {file_name} : {len(scan_ids)}\")\n",
    "    return scan_ids\n",
    "\n",
    "\n",
    "'''\n",
    "description:  从SR3D作者那获取 详细的标注数据\n",
    "param {*} split_name\n",
    "return {*}\n",
    "'''\n",
    "def get_refer_it_3D(split_name='sr3d'):\n",
    "    scanrefer_root=\"datasets/refer_it_3d\"\n",
    "    data = pd.read_csv(osp.join(scanrefer_root,split_name+\".csv\"))\n",
    "    \n",
    "    # logger.info(f\"len of {split_name} : {data.shape[0]}\")\n",
    "    all_attrs = data.columns\n",
    "    # logger.info(f\" column of {split_name} : {all_attrs}\")\n",
    "    # logger.info(f\"scene number : {len(set(data['scan_id']))}\")\n",
    "\n",
    "    stat = Counter(data['scan_id'])\n",
    "\n",
    "    scane_stat = np.array([v for k ,v in stat.items()])\n",
    "    avg_sample =scane_stat.mean()\n",
    "    min_sample =scane_stat.min()\n",
    "    max_sample =scane_stat.max()\n",
    "\n",
    "\n",
    "    # logger.info(f\"min sample: {min_sample} \\n max sample : {max_sample} \\n avg sample: {avg_sample}\")    \n",
    "    # print(data.iloc[0,:])\n",
    "    return data\n",
    "    \n",
    "      \n",
    "\n",
    "def save_txt(path,data):\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(data)\n",
    "    \n",
    "\n",
    "'''\n",
    "description:  \n",
    "param {*} labeled_ratio\n",
    "return {*}\n",
    "'''\n",
    "def generate_NR3D_labeled_scene_txt(labeled_ratio):\n",
    "    nr3d_ids = get_split_list(dataset_name='nr3d')\n",
    "\n",
    "    num_scans = len(nr3d_ids)\n",
    "    num_labeled_scans = int(num_scans*labeled_ratio)\n",
    "\n",
    "\n",
    "    choices = np.random.choice(num_scans, num_labeled_scans, replace=False)#* 从num_scans 挑选num_labeled_scans 个场景 出来 \n",
    "\n",
    "    labeled_scan_names = list(np.array(list(nr3d_ids))[choices])\n",
    "    \n",
    "    save_txt(os.path.join('data/meta_data/nr3d_train_{}.txt'.format(labeled_ratio)),'\\n'.join(labeled_scan_names))\n",
    "   \n",
    "    logger.info('\\tSelected {} labeled scans, remained {} unlabeled scans'.format(len(labeled_scan_names),num_scans- len(labeled_scan_names)))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# sr3d_ids = get_split_list()\n",
    "# sr3d_ids_test = get_split_list(split = 'test')\n",
    "# nr3d_ids = get_split_list(dataset_name='nr3d')\n",
    "# nr3d_ids_test = get_split_list(dataset_name='nr3d',split = 'test')\n",
    "\n",
    "# get_refer_it_3D('sr3d+')\n",
    "# get_refer_it_3D('sr3d')\n",
    "# get_refer_it_3D('sr3d_test')\n",
    "# get_refer_it_3D('sr3d_train')\n",
    "\n",
    "\n",
    "\n",
    "for ratio in np.linspace(0.1,0.9,9):\n",
    "    get_ratio_split_list(round(ratio,1))\n",
    "    # print(ratio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "description:  统计NR3D 数据集\n",
    "return {*}\n",
    "'''\n",
    "def statstic_nr3d():\n",
    "    nr3d = get_refer_it_3D('nr3d')\n",
    "    nr3d_train =  get_split_list(dataset_name='nr3d')\n",
    "    len_test= len_train= 0\n",
    "    for idx in range(nr3d.shape[0]):\n",
    "        if nr3d.iloc[idx,:]['scan_id'] in nr3d_train:\n",
    "            len_train+=1\n",
    "        else:\n",
    "            len_test+=1\n",
    "    print(f\"len of train : {len_train}, len of test : {len_test}\")\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "description:  根据比例统计每个子集\n",
    "param {*} ratio\n",
    "param {*} datasets\n",
    "return {*}\n",
    "'''\n",
    "def statstic_nr3d_by_ratio(ratio,datasets='nr3d'):\n",
    "    \n",
    "    nr3d = get_refer_it_3D(datasets)\n",
    "    nr3d_ratio=  get_ratio_split_list(ratio,dataset_name=datasets)\n",
    "    \n",
    "    num = 0 \n",
    "    for idx in range(nr3d.shape[0]):\n",
    "        if nr3d.iloc[idx,:]['scan_id'] in nr3d_ratio:\n",
    "            num+=1\n",
    "        \n",
    "    print(f\"len of {datasets,ratio} : {num}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_scene_data(all_data,scene = 'scene0525_00'):\n",
    "    \n",
    "\n",
    "    return_data = [] \n",
    "    for  idx in range(all_data.shape[0]):\n",
    "\n",
    "        if all_data.iloc[idx]['scan_id'] == scene:\n",
    "            return_data.append(all_data.iloc[idx])\n",
    "            \n",
    "\n",
    "    return return_data\n",
    "\n",
    "\n",
    "def get_assignment_id(all_data,scene_list):\n",
    "    assignments = []\n",
    "    for idx in range(all_data.shape[0]):\n",
    "        if all_data.iloc[idx]['scan_id'] in scene_list: \n",
    "            assignments.append (all_data.iloc[idx]['assignmentid'])\n",
    "        \n",
    "    return assignments\n",
    "\n",
    "# scene_0525  = get_scene_data(nr3d)\n",
    "# all_assign_ids = get_assignment_id(nr3d)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_labeled_according_assignment_id(assign_ids , ratio=0.4):\n",
    "    \n",
    "    \n",
    "    length = len(assign_ids)\n",
    "    choices = np.random.choice(length,int(length*ratio))\n",
    "\n",
    "    return np.array(assign_ids)[choices]\n",
    "\n",
    "\n",
    "def split_nr3d_according_to_assignmentid():\n",
    "    for ratio in np.linspace(0.1,0.9,9):\n",
    "        ratio = round(ratio,1)\n",
    "        split_labeled_data = split_labeled_according_assignment_id(nr3d_all_assign_ids,ratio)\n",
    "        print(f\"length  = {len(split_labeled_data)}\")\n",
    "        save_txt(os.path.join('data/meta_data/nr3d_train_{}.txt'.format(ratio)),'\\n'.join(split_labeled_data.astype(np.str0).tolist()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stat_mention_data_nr3d(nr3d_all_data,ratio=0.2):\n",
    "    data = read_txt('data/meta_data/nr3d_train_{}.txt'.format(ratio))\n",
    "    sum_=0\n",
    "    for idx in range(nr3d_all_data.shape[0]):\n",
    "        # if str(nr3d.iloc[idx]['assignmentid']) in data and nr3d.iloc[idx]['mentions_target_class']  and nr3d.iloc[idx]['correct_guess'] :\n",
    "        if str(nr3d_all_data.iloc[idx]['assignmentid']) in data and nr3d_all_data.iloc[idx]['mentions_target_class']:\n",
    "            sum_+=1\n",
    "    logger.info(sum_)\n",
    "\n",
    "# nr3d = get_refer_it_3D('nr3d')\n",
    "# train_split = get_split_list(dataset_name='nr3d')\n",
    "# nr3d_all_assign_ids = get_assignment_id(nr3d,train_split)\n",
    "# save_txt(os.path.join('data/meta_data/nr3d_train_all_assignmentid.txt'),'\\n'.join(np.array(nr3d_all_assign_ids).astype(np.str0).tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_scanrefer(split=None):\n",
    "    if split is not None :\n",
    "        path = \"datasets/scanrefer/ScanRefer_filtered_%s.json\"%split\n",
    "    else :\n",
    "        path = \"datasets/scanrefer/ScanRefer_filtered.json\"\n",
    "\n",
    "    with open (path,'r')as f :\n",
    "        data =json.load(f)\n",
    "    \n",
    "    length = len(data)\n",
    "    logger.info(f\" len of {split} split : {length}\")\n",
    "    # all_scene = set([x['scene_id']  for x in data])\n",
    "    # logger.info(f\" scene number  of {split} split : {len(all_scene)}\")\n",
    "\n",
    "    # all_object_id = set([x['object_id']  for x in data])\n",
    "    # logger.info(f\" object number  of {split} split : {len(all_object_id)}\")\n",
    "\n",
    "    # all_anno_id = set([x['ann_id']  for x in data])\n",
    "    # logger.info(f\" anno number  of {split} split : {len(all_anno_id)}\")\n",
    "\n",
    "    # print(data[0])\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_ratio_scanrefer(ratio,split=None):\n",
    "    \n",
    "    path = \"datasets/scanrefer/ScanRefer_filtered_train_%.1f.txt\"%(ratio)\n",
    "\n",
    "    scanrefer = get_scanrefer(split='train')\n",
    "    scan_ids = read_txt(path)\n",
    "\n",
    "    \n",
    "    num = 0 \n",
    "    for refer in scanrefer:\n",
    "        if refer['scene_id'] in scan_ids:\n",
    "            num+=1\n",
    "    \n",
    "    print(f\"len of scanrefer {ratio} : {num}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "description: \n",
    "param {*} labeled_ratio\n",
    "return {*}\n",
    "'''\n",
    "def generate_scanrefer_labeled_scene_txt(labeled_ratio):\n",
    "    all_scenes = get_scanrefer(split='train')\n",
    "\n",
    "    num_scans = len(all_scenes)\n",
    "    num_labeled_scans = int(num_scans*labeled_ratio)\n",
    "\n",
    "\n",
    "    choices = np.random.choice(num_scans, num_labeled_scans, replace=False)#* 从num_scans 挑选num_labeled_scans 个场景 出来 \n",
    "\n",
    "    labeled_scan_names = list(np.array(list(all_scenes))[choices])\n",
    "    \n",
    "    with open(os.path.join('datasets/scanrefer/ScanRefer_filtered_train_{}.txt'.format(labeled_ratio)), 'w') as f:\n",
    "        f.write('\\n'.join(labeled_scan_names))\n",
    "    \n",
    "    logger.info('\\tSelected {} labeled scans, remained {} unlabeled scans'.format(len(labeled_scan_names),num_scans- len(labeled_scan_names)))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "'''\n",
    "description:  一次性生成所有比例的数据集划分\n",
    "return {*}\n",
    "'''\n",
    "def generate_ratio_labeled_datasets():\n",
    "    for x in np.linspace(0.1,0.9,9):\n",
    "        generate_NR3D_labeled_scene_txt(round(x,1))\n",
    "    for x in np.linspace(0.1,0.9,9):\n",
    "        generate_scanrefer_labeled_scene_txt(round(x,1))\n",
    "\n",
    "'''\n",
    "description:  tmp code for stat \n",
    "return {*}\n",
    "'''\n",
    "def stat():\n",
    "    datasets = ['sr3d','nr3d']\n",
    "\n",
    "    for dataset in datasets:\n",
    "        for n in np.linspace(0.1,0.9,9):\n",
    "            # print(round(n,1))\n",
    "            statstic_nr3d_by_ratio(round(n,1),dataset)\n",
    "            \n",
    "    for n in np.linspace(0.1,0.9,9):\n",
    "        get_ratio_scanrefer(round(n,1))\n",
    "        \n",
    "\n",
    "\n",
    "# scan_val =get_scanrefer(split='val')\n",
    "\n",
    "\n",
    "# get_scanrefer(split='test')\n",
    "# get_scanrefer()\n",
    "# scanrefer = get_scanrefer(split='train')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scanrefer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1785247/3171340516.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mscene_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"scene0000_00\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mscene_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_scanrefer_data_by_scene\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscanrefer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscene_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"len of scene : {len(scene_data)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scanrefer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_scanrefer_data_by_scene(all_data,scene_name):\n",
    "    \n",
    "    res = [] \n",
    "    for line in all_data:\n",
    "        # print(line['scene_id'])\n",
    "        # print(line['object_id'])\n",
    "        # print(line['object_name'])\n",
    "        # print(line['ann_id'])\n",
    "\n",
    "        # for k,v in line.items():\n",
    "        #     print(k)\n",
    "        if line['scene_id'] == scene_name:\n",
    "            res.append(line)\n",
    "    return res\n",
    "\n",
    "scene_name = \"scene0000_00\"\n",
    "scene_data = get_scanrefer_data_by_scene(scanrefer,scene_name)\n",
    "print(f\"len of scene : {len(scene_data)}\")\n",
    "print(scene_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of scene : 173\n",
      "{'scene_id': 'scene0000_00', 'object_id': '39', 'object_name': 'cabinet', 'ann_id': '1', 'description': 'a white cabinet in the corner of the room. in the direction from the door and from the inside . it will be on the left, there is a small brown table on the left side of the cabinet and a smaller table on the right side of the cabinet', 'token': ['a', 'white', 'cabinet', 'in', 'the', 'corner', 'of', 'the', 'room', '.', 'in', 'the', 'direction', 'from', 'the', 'door', 'and', 'from', 'the', 'inside', '.', 'it', 'will', 'be', 'on', 'the', 'left', ',', 'there', 'is', 'a', 'small', 'brown', 'table', 'on', 'the', 'left', 'side', 'of', 'the', 'cabinet', 'and', 'a', 'smaller', 'table', 'on', 'the', 'right', 'side', 'of', 'the', 'cabinet']}\n",
      "object_39 : 5\n",
      "ann_1 : 34\n",
      "object_39_ann_1 : 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def stat_object_id_for_scene(scene_data,object_id):\n",
    "    res = [] \n",
    "    for line in scene_data:\n",
    "        if line['object_id'] == object_id:\n",
    "            res.append(line)\n",
    "    return res\n",
    "\n",
    "def stat_ann_id_for_scene(scene_data,ann_id):\n",
    "    res = [] \n",
    "    for line in scene_data:\n",
    "        if line['ann_id'] == ann_id:\n",
    "            res.append(line)\n",
    "    return res\n",
    "\n",
    "def stat_ann_id_and_object_id_for_scene(scene_data,ann_id,object_id):\n",
    "    res = [] \n",
    "    for line in scene_data:\n",
    "        if line['ann_id'] == ann_id and line['object_id'] == object_id:\n",
    "            res.append(line)\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "# object_39 = stat_object_id_for_scene(scene_data,'39')\n",
    "# ann_1=stat_ann_id_for_scene(scene_data,'1')\n",
    "# object_39_ann_1 = stat_ann_id_and_object_id_for_scene(scene_data,'1','39')\n",
    "# print(f\"object_39 : {len(object_39)}\")\n",
    "# print(f\"ann_1 : {len(ann_1)}\")\n",
    "# print(f\"object_39_ann_1 : {len(object_39_ann_1)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('cerberus2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c7eafccb8f457aa8ce756ee28bedbf2f0acb240aef93477faad38317e939d03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
